# ğŸ¯ GRM PROJESÄ°: BÃœTÃœNCÃœL GELÄ°ÅTÄ°RME Ã–NERÄ°LERÄ°

**Tarih:** 2025-11-15  
**Analiz:** Mevcut Proje + Hipotez_03 + Hipotez_04 (PIML)  
**Hedef:** Daha Rasyonel ve Ä°yi Ã‡Ä±ktÄ±lar Ãœreten Sistem

---

## ğŸ“Š MEVCUT DURUMUN ANALÄ°ZÄ°

### âœ… BaÅŸarÄ±lan AdÄ±mlar

1. **3-FazlÄ± YaklaÅŸÄ±m Ä°mplementasyonu**
   - Faze 1: Sentetik veri + Schwarzschild âœ…
   - Faze 2: Kerr rejimi + Non-linear bÃ¼kÃ¼lme âœ…
   - Faze 3: GerÃ§ek veri + GARCH karÅŸÄ±laÅŸtÄ±rma âœ…

2. **Metodolojik SaÄŸlamlÄ±k**
   - Data leakage dÃ¼zeltildi âœ…
   - Walk-forward validation eklendi âœ…
   - Proper time-series split âœ…
   - Ä°statistiksel testler (Diebold-Mariano, ARCH-LM) âœ…

3. **ModÃ¼ler Mimari**
   - Temiz kod yapÄ±sÄ± (PEP8/PEP257) âœ…
   - AyrÄ± model sÄ±nÄ±flarÄ± âœ…
   - Konfigurasyon yÃ¶netimi âœ…

### âš ï¸ Mevcut Limitasyonlar

#### 1. **Ad-Hoc BÃ¼kÃ¼lme Fonksiyonu**
```python
# Mevcut: Manuel tasarÄ±m
Î“(t) = Î± * M(t) * sign(Îµ(t))  # Schwarzschild
Î“(t) = Î± * M(t) * tanh(Îµ(t)) + Î³ * a(t) * Îµ(t)  # Kerr
```

**Sorun:**
- Fonksiyon formu keyfi olarak seÃ§ilmiÅŸ
- Veri kendi dinamiklerini Ã¶ÄŸrenemiyor
- FarklÄ± varlÄ±klar iÃ§in genellenemeyebilir

**Etki:**
- Baseline RMSE: 0.101398
- GRM RMSE: 0.102091
- **GRM daha kÃ¶tÃ¼ performans gÃ¶steriyor**

#### 2. **Ä°ki AÅŸamalÄ± AyrÄ±k Model**
```python
# Mevcut: Pipeline approach
1. Baseline model eÄŸit
2. ArtÄ±klarÄ± hesapla
3. GRM parametrelerini optimize et
4. Tahminleri birleÅŸtir
```

**Sorun:**
- Baseline, GRM'den habersiz
- Tek yÃ¶nlÃ¼ bilgi akÄ±ÅŸÄ±
- Global optimum yerine lokal optimum

#### 3. **Tek Anomali VarsayÄ±mÄ±**
- TÃ¼m artÄ±klar tek bir "kara delik" tarafÄ±ndan aÃ§Ä±klanÄ±yor
- FarklÄ± ÅŸok kaynaklarÄ± (pozitif/negatif, kÄ±sa/uzun dÃ¶nem) modellenmemiÅŸ

#### 4. **SÄ±nÄ±rlÄ± Ablasyon Ã‡alÄ±ÅŸmasÄ±**
- Hangi bileÅŸenin ne kadar katkÄ± saÄŸladÄ±ÄŸÄ± net deÄŸil
- Parametre hassasiyeti sistematik olarak test edilmemiÅŸ

---

## ğŸ¯ Ã–NCELÄ°KLÄ° GELÄ°ÅTÄ°RME Ã–NERÄ°LERÄ°

### ğŸ¥‡ **Ã–NCELÄ°K 1: ZenginleÅŸtirilmiÅŸ BÃ¼kÃ¼lme Fonksiyonu (Hipotez_03)**

**Ne:** Decay factor (Ï„) ve geliÅŸmiÅŸ metrik seÃ§imi ekle

**Neden:** 
- ÅoklarÄ±n etkisi zamanla azalmalÄ± (fiziksel olarak tutarlÄ±)
- Olay ufku istatistiksel olarak tanÄ±mlanmalÄ±

**NasÄ±l:**

```python
# MEVCUT (models/grm_model.py):
def compute_curvature(self, residuals, mass):
    return self.alpha * mass * np.sign(residuals)

# Ã–NERÄ°LEN:
def compute_curvature_with_decay(self, residuals, mass, time_since_shock):
    """
    Decay factor eklenmiÅŸ bÃ¼kÃ¼lme fonksiyonu.
    
    Parameters
    ----------
    residuals : array-like
        ArtÄ±k dizisi
    mass : array-like
        KÃ¼tle (volatilite) dizisi
    time_since_shock : array-like
        Her zaman noktasÄ± iÃ§in son bÃ¼yÃ¼k ÅŸoktan geÃ§en zaman
        
    Returns
    -------
    curvature : array-like
        BÃ¼kÃ¼lme dÃ¼zeltmeleri
    """
    # Decay factor: 1 / (1 + Î² * Ï„)
    decay = 1.0 / (1.0 + self.beta * time_since_shock)
    
    # Base curvature
    base_curvature = self.alpha * mass * np.tanh(residuals)
    
    # With decay
    curvature = base_curvature * decay
    
    return curvature

def detect_shocks(self, residuals, threshold_quantile=0.95):
    """
    BÃ¼yÃ¼k ÅŸoklarÄ± tespit et (olay ufku analojisi).
    
    Parameters
    ----------
    residuals : array-like
        ArtÄ±k dizisi
    threshold_quantile : float
        Åok eÅŸiÄŸi (Ã¶rn: %95 quantile)
        
    Returns
    -------
    shock_times : list
        Åok zamanlarÄ±nÄ±n indeksleri
    """
    abs_residuals = np.abs(residuals)
    threshold = np.quantile(abs_residuals, threshold_quantile)
    shock_times = np.where(abs_residuals > threshold)[0]
    return shock_times

def compute_time_since_shock(self, current_time, shock_times):
    """
    Her zaman noktasÄ± iÃ§in son ÅŸoktan geÃ§en zamanÄ± hesapla.
    
    Parameters
    ----------
    current_time : int
        GÃ¼ncel zaman indeksi
    shock_times : list
        Åok zamanlarÄ±nÄ±n indeksleri
        
    Returns
    -------
    tau : float
        Son ÅŸoktan geÃ§en zaman (adÄ±m sayÄ±sÄ±)
    """
    if len(shock_times) == 0 or current_time < shock_times[0]:
        return float('inf')  # HiÃ§ ÅŸok olmadÄ±
    
    past_shocks = shock_times[shock_times < current_time]
    if len(past_shocks) == 0:
        return float('inf')
    
    last_shock = past_shocks[-1]
    tau = current_time - last_shock
    return tau
```

**Beklenen Ä°yileÅŸme:**
- RMSE: %2-5 iyileÅŸme
- Fiziksel tutarlÄ±lÄ±k: â­â­â­â­â­
- Implementasyon sÃ¼resi: 1-2 gÃ¼n

**Action Plan:**
1. `models/grm_model.py` ve `models/kerr_grm_model.py` gÃ¼ncelle
2. `config_phase3.py` iÃ§ine `decay_beta_range` ekle
3. `main_phase3.py` iÃ§inde decay parametresi optimize et
4. SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r (eski vs yeni)

---

### ğŸ¥ˆ **Ã–NCELÄ°K 2: KapsamlÄ± Ablasyon ve Hassasiyet Ã‡alÄ±ÅŸmasÄ± (Hipotez_03)**

**Ne:** Her bileÅŸenin katkÄ±sÄ±nÄ± sistematik olarak Ã¶lÃ§

**Neden:**
- Hangi parametrenin kritik olduÄŸunu anla
- Gereksiz karmaÅŸÄ±klÄ±ktan kaÃ§Ä±n
- Model yorumlanabilirliÄŸini artÄ±r

**NasÄ±l:**

```python
# Yeni dosya: models/ablation_study.py

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
from models import BaselineARIMA, SchwarzschildGRM, KerrGRM
from models.metrics import calculate_rmse

class AblationStudy:
    """
    GRM modeli iÃ§in kapsamlÄ± ablasyon Ã§alÄ±ÅŸmasÄ±.
    
    Test edilen varyasyonlar:
    1. Sadece kÃ¼tle (M) - dÃ¶nme yok
    2. Sadece dÃ¶nme (a) - kÃ¼tle yok
    3. Decay yok (Î²=0)
    4. Non-linearity yok (tanh -> sign)
    5. FarklÄ± pencere boyutlarÄ±
    """
    
    def __init__(self, train_data, val_data, test_data):
        self.train_data = train_data
        self.val_data = val_data
        self.test_data = test_data
        self.results = {}
    
    def run_baseline(self):
        """Baseline model (karÅŸÄ±laÅŸtÄ±rma referansÄ±)."""
        baseline = BaselineARIMA()
        baseline.fit(self.train_data)
        predictions = baseline.predict(len(self.test_data))
        rmse = calculate_rmse(self.test_data, predictions)
        self.results['Baseline'] = {'rmse': rmse, 'components': []}
        return rmse
    
    def run_variant(self, name: str, model_class, **kwargs):
        """Bir GRM varyantÄ±nÄ± Ã§alÄ±ÅŸtÄ±r."""
        # Baseline fit
        baseline = BaselineARIMA()
        baseline.fit(self.train_data)
        train_residuals = baseline.get_residuals()
        
        # GRM fit
        grm_model = model_class(**kwargs)
        grm_model.fit(train_residuals)
        
        # Test predictions (walk-forward)
        # ... (implement walk-forward logic)
        
        rmse = calculate_rmse(self.test_data, predictions)
        self.results[name] = {
            'rmse': rmse,
            'components': list(kwargs.keys()),
            'improvement': (self.results['Baseline']['rmse'] - rmse) / self.results['Baseline']['rmse'] * 100
        }
        return rmse
    
    def test_mass_only(self):
        """Ablasyon 1: Sadece kÃ¼tle (M), dÃ¶nme yok."""
        return self.run_variant(
            name='Mass_Only',
            model_class=SchwarzschildGRM,
            window_size=20,
            use_decay=True
        )
    
    def test_spin_only(self):
        """Ablasyon 2: Sadece dÃ¶nme (a), kÃ¼tle sabit."""
        # Custom variant needed
        pass
    
    def test_no_decay(self):
        """Ablasyon 3: Decay yok (Î²=0)."""
        return self.run_variant(
            name='No_Decay',
            model_class=KerrGRM,
            window_size=20,
            use_decay=False,
            use_tanh=True
        )
    
    def test_linear_only(self):
        """Ablasyon 4: Non-linearity yok (sign yerine tanh)."""
        return self.run_variant(
            name='Linear_Only',
            model_class=KerrGRM,
            window_size=20,
            use_decay=True,
            use_tanh=False
        )
    
    def test_window_sizes(self, sizes: List[int] = [10, 20, 30, 50, 100]):
        """Hassasiyet 1: FarklÄ± pencere boyutlarÄ±."""
        for w in sizes:
            self.run_variant(
                name=f'Window_{w}',
                model_class=KerrGRM,
                window_size=w,
                use_decay=True,
                use_tanh=True
            )
    
    def test_alpha_sensitivity(self, alphas: List[float] = [0.1, 0.5, 1.0, 2.0, 5.0]):
        """Hassasiyet 2: Alpha parametresi hassasiyeti."""
        for alpha in alphas:
            # Fix alpha, optimize others
            pass
    
    def generate_report(self) -> pd.DataFrame:
        """Ablasyon sonuÃ§larÄ±nÄ± raporla."""
        df = pd.DataFrame.from_dict(self.results, orient='index')
        df = df.sort_values('improvement', ascending=False)
        
        print("\n" + "="*80)
        print("ABLASYON Ã‡ALIÅMASI SONUÃ‡LARI")
        print("="*80)
        print(df.to_string())
        print("\n")
        
        # En iyi ve en kÃ¶tÃ¼ bileÅŸenleri bul
        best = df.iloc[0]
        worst = df.iloc[-1]
        
        print(f"EN Ä°YÄ° VARÄ°YASYON: {best.name}")
        print(f"  - RMSE: {best['rmse']:.6f}")
        print(f"  - Ä°yileÅŸme: {best['improvement']:.2f}%")
        print(f"  - BileÅŸenler: {best['components']}")
        print()
        
        return df
    
    def plot_results(self):
        """Ablasyon sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir."""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. RMSE karÅŸÄ±laÅŸtÄ±rmasÄ±
        names = list(self.results.keys())
        rmses = [self.results[n]['rmse'] for n in names]
        axes[0, 0].bar(names, rmses)
        axes[0, 0].set_title('RMSE KarÅŸÄ±laÅŸtÄ±rmasÄ±')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Ä°yileÅŸme yÃ¼zdeleri
        improvements = [self.results[n].get('improvement', 0) for n in names]
        axes[0, 1].barh(names, improvements)
        axes[0, 1].set_title('Baseline\'a GÃ¶re Ä°yileÅŸme (%)')
        axes[0, 1].axvline(0, color='red', linestyle='--')
        
        # 3. Pencere boyutu hassasiyeti
        # ...
        
        # 4. BileÅŸen katkÄ±larÄ±
        # ...
        
        plt.tight_layout()
        plt.savefig('results/ablation_study.png', dpi=300, bbox_inches='tight')
        plt.close()
```

**KullanÄ±m:**

```python
# main_ablation_study.py

from models.ablation_study import AblationStudy

# Veri hazÄ±rla
train_df, val_df, test_df = split_data(df)

# Ablasyon Ã§alÄ±ÅŸmasÄ±
study = AblationStudy(train_df['y'], val_df['y'], test_df['y'])

# Baseline
study.run_baseline()

# Ablasyonlar
study.test_mass_only()
study.test_spin_only()
study.test_no_decay()
study.test_linear_only()

# Hassasiyet analizleri
study.test_window_sizes()
study.test_alpha_sensitivity()

# Rapor
results_df = study.generate_report()
study.plot_results()
```

**Beklenen Ã‡Ä±ktÄ±:**
```
================================================================================
ABLASYON Ã‡ALIÅMASI SONUÃ‡LARI
================================================================================
                    rmse  components                      improvement
Kerr_Full      0.098234  [M, a, decay, tanh]            +3.12%
Schwarzschild  0.100123  [M, decay, tanh]               +1.26%
Mass_Only      0.101456  [M]                            +0.06%
No_Decay       0.102789  [M, a, tanh]                   -1.37%
Linear_Only    0.103234  [M, a, decay]                  -1.81%
Baseline       0.101398  []                             0.00%

EN Ä°YÄ° VARÄ°YASYON: Kerr_Full
  - RMSE: 0.098234
  - Ä°yileÅŸme: +3.12%
  - BileÅŸenler: ['M', 'a', 'decay', 'tanh']
```

**Action Plan:**
1. `models/ablation_study.py` oluÅŸtur
2. `main_ablation_study.py` oluÅŸtur
3. TÃ¼m varyasyonlarÄ± Ã§alÄ±ÅŸtÄ±r (4-6 saat hesaplama)
4. SonuÃ§larÄ± analiz et ve rapor oluÅŸtur

---

### ğŸ¥‰ **Ã–NCELÄ°K 3: Time-Series Cross-Validation (Hipotez_03)**

**Ne:** Tek test seti yerine rolling window validation

**Neden:**
- Model saÄŸlamlÄ±ÄŸÄ±nÄ± farklÄ± dÃ¶nemlerde test et
- AÅŸÄ±rÄ± uydurma (overfitting) tespiti
- Daha gÃ¼venilir performans tahmini

**NasÄ±l:**

```python
# Yeni dosya: models/cross_validation.py

import numpy as np
from typing import List, Tuple, Dict

class TimeSeriesCrossValidator:
    """
    Time-series iÃ§in walk-forward cross-validation.
    
    Strateji:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Fold 1: [Trainâ”€â”€â”€â”€â”€â”€â”€â”€][Valâ”€â”€][Testâ”€â”€]                 â”‚
    â”‚ Fold 2:    [Trainâ”€â”€â”€â”€â”€â”€â”€â”€][Valâ”€â”€][Testâ”€â”€]              â”‚
    â”‚ Fold 3:       [Trainâ”€â”€â”€â”€â”€â”€â”€â”€][Valâ”€â”€][Testâ”€â”€]           â”‚
    â”‚ Fold 4:          [Trainâ”€â”€â”€â”€â”€â”€â”€â”€][Valâ”€â”€][Testâ”€â”€]        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    def __init__(
        self,
        initial_train_size: int = 300,
        val_size: int = 50,
        test_size: int = 50,
        step_size: int = 50
    ):
        self.initial_train_size = initial_train_size
        self.val_size = val_size
        self.test_size = test_size
        self.step_size = step_size
    
    def split(self, data: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        Veriyi k fold'a bÃ¶l.
        
        Returns
        -------
        folds : List[Tuple]
            Her fold iÃ§in (train_indices, val_indices, test_indices)
        """
        n = len(data)
        folds = []
        
        current_train_end = self.initial_train_size
        
        while current_train_end + self.val_size + self.test_size <= n:
            train_indices = np.arange(0, current_train_end)
            val_indices = np.arange(current_train_end, current_train_end + self.val_size)
            test_indices = np.arange(
                current_train_end + self.val_size,
                current_train_end + self.val_size + self.test_size
            )
            
            folds.append((train_indices, val_indices, test_indices))
            current_train_end += self.step_size
        
        return folds
    
    def evaluate_model(
        self,
        model_class,
        data: np.ndarray,
        **model_kwargs
    ) -> Dict[str, List[float]]:
        """
        Modeli tÃ¼m fold'larda deÄŸerlendir.
        
        Returns
        -------
        results : Dict
            Her metrik iÃ§in fold sonuÃ§larÄ±
        """
        folds = self.split(data)
        results = {
            'rmse': [],
            'mae': [],
            'fold': []
        }
        
        for i, (train_idx, val_idx, test_idx) in enumerate(folds):
            print(f"  Fold {i+1}/{len(folds)}...")
            
            train_data = data[train_idx]
            val_data = data[val_idx]
            test_data = data[test_idx]
            
            # Model train
            model = model_class(**model_kwargs)
            model.fit(train_data, val_data)
            
            # Test predict
            predictions = model.predict(len(test_data))
            
            # Metrics
            rmse = np.sqrt(np.mean((test_data - predictions) ** 2))
            mae = np.mean(np.abs(test_data - predictions))
            
            results['rmse'].append(rmse)
            results['mae'].append(mae)
            results['fold'].append(i + 1)
        
        return results
    
    def compare_models(
        self,
        models: Dict[str, Tuple[type, dict]],
        data: np.ndarray
    ) -> pd.DataFrame:
        """
        Birden fazla modeli karÅŸÄ±laÅŸtÄ±r.
        
        Parameters
        ----------
        models : Dict[str, Tuple[type, dict]]
            Model adÄ± -> (model_class, model_kwargs)
        data : np.ndarray
            Zaman serisi verisi
            
        Returns
        -------
        comparison_df : pd.DataFrame
            KarÅŸÄ±laÅŸtÄ±rma tablosu
        """
        all_results = {}
        
        for name, (model_class, kwargs) in models.items():
            print(f"\n{name} deÄŸerlendiriliyor...")
            results = self.evaluate_model(model_class, data, **kwargs)
            all_results[name] = results
        
        # Ã–zet istatistikler
        summary = []
        for name, results in all_results.items():
            summary.append({
                'Model': name,
                'Mean_RMSE': np.mean(results['rmse']),
                'Std_RMSE': np.std(results['rmse']),
                'Min_RMSE': np.min(results['rmse']),
                'Max_RMSE': np.max(results['rmse']),
                'Mean_MAE': np.mean(results['mae']),
                'Std_MAE': np.std(results['mae'])
            })
        
        df = pd.DataFrame(summary)
        df = df.sort_values('Mean_RMSE')
        
        return df, all_results
```

**KullanÄ±m:**

```python
# main_cross_validation.py

from models.cross_validation import TimeSeriesCrossValidator

# CV oluÅŸtur
cv = TimeSeriesCrossValidator(
    initial_train_size=300,
    val_size=50,
    test_size=50,
    step_size=50
)

# Modelleri tanÄ±mla
models = {
    'Baseline': (BaselineARIMA, {}),
    'Schwarzschild': (SchwarzschildGRM, {'window_size': 20}),
    'Kerr': (KerrGRM, {'window_size': 20, 'use_tanh': True}),
    'GARCH': (GARCHModel, {'p': 1, 'q': 1})
}

# KarÅŸÄ±laÅŸtÄ±r
comparison_df, detailed_results = cv.compare_models(models, df['y'].values)

print("\n" + "="*80)
print("TIME-SERIES CROSS-VALIDATION SONUÃ‡LARI")
print("="*80)
print(comparison_df.to_string())
```

**Beklenen Ã‡Ä±ktÄ±:**
```
================================================================================
TIME-SERIES CROSS-VALIDATION SONUÃ‡LARI
================================================================================
          Model  Mean_RMSE  Std_RMSE  Min_RMSE  Max_RMSE  Mean_MAE  Std_MAE
0          Kerr    0.09823   0.01234   0.08456   0.11234   0.07234  0.00987
1  Schwarzschild  0.10012   0.01456   0.08789   0.11567   0.07456  0.01123
2      Baseline    0.10140   0.01567   0.08923   0.12345   0.07567  0.01234
3         GARCH    0.10170   0.01678   0.09012   0.12456   0.07678  0.01345
```

**Action Plan:**
1. `models/cross_validation.py` oluÅŸtur
2. `main_cross_validation.py` oluÅŸtur
3. TÃ¼m modelleri CV ile deÄŸerlendir (6-8 saat)
4. SonuÃ§larÄ± raporla

---

## ğŸš€ UZUN VADELÄ° GELÄ°ÅTÄ°RMELER (PIML - Hipotez_04)

### ğŸ”¬ **GELÄ°ÅTÄ°RME 1: Gravitational Residual Network (GRN)**

**Ne:** BÃ¼kÃ¼lme fonksiyonunu Ã¶ÄŸrenen bir sinir aÄŸÄ±

**Teorik Temel:**
- PINN benzeri yaklaÅŸÄ±m
- Physics-informed inductive bias
- Ã–ÄŸrenilebilir parametreler

**Mimari:**

```python
# Yeni dosya: models/grn_network.py

import torch
import torch.nn as nn
import numpy as np

class GravitationalResidualNetwork(nn.Module):
    """
    Physics-inspired neural network for learning curvature function.
    
    Architecture:
        Input: [M(t), a(t), Ï„(t), Îµ(t-k:t)] â†’ hidden layers â†’ Output: Î“(t+1)
    
    Physics-informed constraints:
        1. Monotonicity: âˆ‚Î“/âˆ‚M â‰¥ 0 (larger mass â†’ larger curvature)
        2. Energy conservation: Î£|Î“(t)| is bounded
        3. Symmetry: Î“(M, a, Ï„) = -Î“(M, -a, Ï„) for spin
    """
    
    def __init__(
        self,
        input_size: int = 4,
        hidden_sizes: List[int] = [64, 32, 16],
        output_size: int = 1,
        use_monotonicity: bool = True,
        use_energy_conservation: bool = True
    ):
        super().__init__()
        
        self.use_monotonicity = use_monotonicity
        self.use_energy_conservation = use_energy_conservation
        
        # Encoder network
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        layers.append(nn.Tanh())  # Bounded output [-1, 1]
        
        self.network = nn.Sequential(*layers)
        
        # Learnable physics parameters
        self.alpha = nn.Parameter(torch.tensor(1.0))
        self.beta = nn.Parameter(torch.tensor(0.1))
        self.gamma = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, mass, spin, tau, residuals_history):
        """
        Forward pass.
        
        Parameters
        ----------
        mass : torch.Tensor, shape (batch, 1)
            KÃ¼tle (volatilite)
        spin : torch.Tensor, shape (batch, 1)
            DÃ¶nme (otokorelasyon)
        tau : torch.Tensor, shape (batch, 1)
            Åoktan geÃ§en zaman
        residuals_history : torch.Tensor, shape (batch, seq_len)
            GeÃ§miÅŸ artÄ±klar dizisi
            
        Returns
        -------
        curvature : torch.Tensor, shape (batch, 1)
            BÃ¼kÃ¼lme dÃ¼zeltmesi
        """
        # Decay factor
        decay = 1.0 / (1.0 + self.beta * tau)
        
        # Input features
        x = torch.cat([mass, spin, tau, residuals_history[:, -1:]], dim=1)
        
        # Neural network correction
        nn_correction = self.network(x)
        
        # Physics-inspired base term
        base_term = self.alpha * mass * torch.tanh(residuals_history[:, -1:])
        spin_term = self.gamma * spin * residuals_history[:, -1:]
        
        # Combined output
        curvature = (base_term + spin_term + nn_correction) * decay
        
        return curvature
    
    def physics_loss(self, mass, curvature):
        """
        Physics-informed loss term.
        
        Enforces:
        1. Monotonicity: dÎ“/dM â‰¥ 0
        2. Energy conservation: Total energy bounded
        """
        loss = 0.0
        
        if self.use_monotonicity:
            # Monotonicity constraint
            # Approximate derivative using finite differences
            mass_perturbed = mass + 0.01
            curvature_perturbed = self.forward(mass_perturbed, ...)
            
            derivative = (curvature_perturbed - curvature) / 0.01
            monotonicity_loss = torch.relu(-derivative).mean()  # Penalize negative derivatives
            loss += 0.1 * monotonicity_loss
        
        if self.use_energy_conservation:
            # Energy conservation: penalize large total energy
            total_energy = torch.sum(torch.abs(curvature))
            energy_loss = torch.relu(total_energy - 10.0)  # Soft threshold
            loss += 0.01 * energy_loss
        
        return loss
    
    def combined_loss(self, predictions, targets, mass, curvature):
        """
        Combined loss: Data fidelity + Physics-informed.
        
        L_total = L_data + Î» * L_physics
        """
        # Data fidelity loss
        data_loss = nn.MSELoss()(predictions, targets)
        
        # Physics loss
        physics_loss = self.physics_loss(mass, curvature)
        
        # Combined
        total_loss = data_loss + 0.1 * physics_loss
        
        return total_loss, data_loss, physics_loss
```

**Training Loop:**

```python
# Yeni dosya: models/grn_trainer.py

class GRNTrainer:
    """GRN model trainer with physics-informed loss."""
    
    def __init__(self, model, learning_rate=0.001):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    def train_epoch(self, train_loader):
        """Train for one epoch."""
        self.model.train()
        total_loss = 0.0
        
        for batch in train_loader:
            mass, spin, tau, residuals_history, targets = batch
            
            # Forward pass
            predictions = self.model(mass, spin, tau, residuals_history)
            
            # Loss
            loss, data_loss, physics_loss = self.model.combined_loss(
                predictions, targets, mass, predictions
            )
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)
    
    def evaluate(self, val_loader):
        """Evaluate on validation set."""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                mass, spin, tau, residuals_history, targets = batch
                predictions = self.model(mass, spin, tau, residuals_history)
                loss, _, _ = self.model.combined_loss(
                    predictions, targets, mass, predictions
                )
                total_loss += loss.item()
        
        return total_loss / len(val_loader)
    
    def fit(self, train_loader, val_loader, epochs=100, early_stopping=10):
        """Full training loop with early stopping."""
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            train_loss = self.train_epoch(train_loader)
            val_loss = self.evaluate(val_loader)
            
            print(f"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'models/grn_best.pth')
            else:
                patience_counter += 1
                if patience_counter >= early_stopping:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
        
        # Load best model
        self.model.load_state_dict(torch.load('models/grn_best.pth'))
```

**Beklenen Ä°yileÅŸme:**
- Manuel fonksiyondan %5-10 daha iyi
- FarklÄ± varlÄ±klara genellenebilir
- Fiziksel kÄ±sÄ±tlamalar sayesinde yorumlanabilir

**Action Plan:**
1. PyTorch kur: `pip install torch`
2. `models/grn_network.py` oluÅŸtur
3. `models/grn_trainer.py` oluÅŸtur
4. `main_grn_train.py` oluÅŸtur ve eÄŸit
5. Manuel fonksiyon vs GRN karÅŸÄ±laÅŸtÄ±r

**SÃ¼re:** 1-2 hafta (veri hazÄ±rlama + eÄŸitim + test)

---

### ğŸŒŒ **GELÄ°ÅTÄ°RME 2: UÃ§tan Uca BirleÅŸik Model (End-to-End)**

**Ne:** Baseline + GRM'yi tek bir modelde birleÅŸtir

**Teorik Temel:**
- PINN-style joint training
- Baseline ve GRM birbirinden Ã¶ÄŸrenir
- Global optimum

**Mimari:**

```python
# Yeni dosya: models/unified_grm.py

class UnifiedGRM(nn.Module):
    """
    End-to-end unified model: Baseline + GRM in one network.
    
    Architecture:
        Input: X(t-k:t) â†’ [LSTM Baseline] â†’ Å¶(t)
                       â†“
                  [Residuals]
                       â†“
        [GRN Network] â†’ Î“(t)
                       â†“
        Final: Å¶(t) + Î“(t)
    
    Loss: L = L_data(Y, Å¶+Î“) + Î»â‚*L_baseline(Y, Å¶) + Î»â‚‚*L_physics(Î“)
    """
    
    def __init__(
        self,
        input_size: int = 1,
        lstm_hidden_size: int = 64,
        grn_hidden_sizes: List[int] = [32, 16]
    ):
        super().__init__()
        
        # Baseline LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=lstm_hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        self.lstm_output = nn.Linear(lstm_hidden_size, 1)
        
        # GRN for residual correction
        self.grn = GravitationalResidualNetwork(
            input_size=4,  # M, a, Ï„, Îµ
            hidden_sizes=grn_hidden_sizes,
            output_size=1
        )
    
    def forward(self, x_history):
        """
        Unified forward pass.
        
        Parameters
        ----------
        x_history : torch.Tensor, shape (batch, seq_len, 1)
            Historical time series
            
        Returns
        -------
        baseline_pred : torch.Tensor
            Baseline LSTM prediction
        grm_correction : torch.Tensor
            GRM correction
        final_pred : torch.Tensor
            Final combined prediction
        """
        # Baseline LSTM prediction
        lstm_out, _ = self.lstm(x_history)
        baseline_pred = self.lstm_output(lstm_out[:, -1, :])
        
        # Compute residuals (from training data, approximated)
        # In real implementation, this needs historical residuals
        residuals = x_history[:, :, 0] - baseline_pred.detach()
        
        # Compute GRM features
        mass = torch.var(residuals, dim=1, keepdim=True)
        spin = self.compute_autocorr(residuals)
        tau = torch.ones_like(mass) * 5.0  # Simplified
        
        # GRM correction
        grm_correction = self.grn(mass, spin, tau, residuals)
        
        # Final prediction
        final_pred = baseline_pred + grm_correction
        
        return baseline_pred, grm_correction, final_pred
    
    def compute_autocorr(self, residuals):
        """Compute lag-1 autocorrelation."""
        # Simplified implementation
        r1 = residuals[:, 1:]
        r0 = residuals[:, :-1]
        corr = torch.mean(r1 * r0, dim=1, keepdim=True) / (torch.var(residuals, dim=1, keepdim=True) + 1e-8)
        return corr
    
    def combined_loss(self, baseline_pred, grm_correction, final_pred, targets):
        """
        Three-component loss function.
        
        L_total = L_final + Î»â‚*L_baseline + Î»â‚‚*L_physics
        """
        # Main loss: final prediction vs targets
        loss_final = nn.MSELoss()(final_pred, targets)
        
        # Baseline loss: encourage baseline to be reasonable
        loss_baseline = nn.MSELoss()(baseline_pred, targets)
        
        # Physics loss: GRM constraints
        mass = torch.var(residuals, dim=1, keepdim=True)  # Need proper implementation
        loss_physics = self.grn.physics_loss(mass, grm_correction)
        
        # Weighted combination
        total_loss = loss_final + 0.1 * loss_baseline + 0.05 * loss_physics
        
        return total_loss, loss_final, loss_baseline, loss_physics
```

**Avantajlar:**
1. **Joint Optimization:** Baseline ve GRM birlikte optimize edilir
2. **Information Flow:** Ä°ki yÃ¶nlÃ¼ bilgi akÄ±ÅŸÄ±
3. **End-to-End Learning:** Global optimum arayÄ±ÅŸÄ±

**Beklenen Ä°yileÅŸme:**
- Manuel + ayrÄ±k yaklaÅŸÄ±mdan %10-15 daha iyi
- Daha stabil tahminler
- Daha iyi genelleme

**Action Plan:**
1. `models/unified_grm.py` oluÅŸtur
2. EÄŸitim pipeline kur
3. AyrÄ±k model vs Unified model karÅŸÄ±laÅŸtÄ±r

**SÃ¼re:** 2-3 hafta

---

### ğŸ§¬ **GELÄ°ÅTÄ°RME 3: Symbolic Regression ile Dinamik KeÅŸfi**

**Ne:** Veriden optimal bÃ¼kÃ¼lme fonksiyonunu otomatik keÅŸfet

**Teorik Temel:**
- AI Feynman, PySR
- Genetic programming
- Interpretable formulas

**Ä°mplementasyon:**

```python
# Yeni dosya: models/symbolic_discovery.py

from pysr import PySRRegressor
import numpy as np

class SymbolicGRM:
    """
    Symbolic regression ile bÃ¼kÃ¼lme fonksiyonunu keÅŸfet.
    
    PySR kullanarak en iyi sembolik denklemi bul:
    Î“(t) = f(M(t), a(t), Ï„(t), Îµ(t))
    """
    
    def __init__(self):
        self.model = PySRRegressor(
            niterations=100,
            binary_operators=["+", "-", "*", "/"],
            unary_operators=["exp", "log", "sqrt", "tanh", "abs"],
            model_selection="best",
            loss="loss(x, y) = (x - y)^2",
            maxsize=20,
            populations=15
        )
    
    def prepare_features(self, residuals, window_size=20):
        """
        GRM feature'larÄ±nÄ± hazÄ±rla.
        
        Returns
        -------
        X : np.ndarray, shape (n_samples, 4)
            [M(t), a(t), Ï„(t), Îµ(t)]
        y : np.ndarray, shape (n_samples,)
            Hedef: gelecekteki artÄ±k veya dÃ¼zeltme
        """
        n = len(residuals)
        X = []
        y = []
        
        for t in range(window_size, n - 1):
            window = residuals[t - window_size:t]
            
            # Features
            mass = np.var(window)
            spin = np.corrcoef(window[1:], window[:-1])[0, 1] if len(window) > 1 else 0.0
            tau = self.compute_tau(residuals[:t], threshold=2.0)
            epsilon = residuals[t]
            
            X.append([mass, spin, tau, epsilon])
            
            # Target: next residual or ideal correction
            y.append(residuals[t + 1])
        
        return np.array(X), np.array(y)
    
    def compute_tau(self, residuals, threshold=2.0):
        """Time since last shock."""
        abs_res = np.abs(residuals)
        shock_indices = np.where(abs_res > threshold)[0]
        
        if len(shock_indices) == 0:
            return len(residuals)
        
        last_shock = shock_indices[-1]
        tau = len(residuals) - last_shock
        return tau
    
    def discover_formula(self, residuals, window_size=20):
        """
        Sembolik denklemi keÅŸfet.
        
        Returns
        -------
        best_formula : str
            En iyi sembolik denklem (e.g., "0.5*M*tanh(epsilon) + 0.1*a*epsilon")
        """
        # Feature hazÄ±rlama
        X, y = self.prepare_features(residuals, window_size)
        
        # Feature isimleri
        feature_names = ["M", "a", "tau", "epsilon"]
        
        # Symbolic regression
        print("Sembolik regresyon baÅŸlatÄ±lÄ±yor...")
        print("(Bu iÅŸlem 10-30 dakika sÃ¼rebilir)")
        
        self.model.fit(X, y, variable_names=feature_names)
        
        # En iyi formÃ¼l
        best_formula = self.model.get_best()
        
        print("\n" + "="*80)
        print("KEÅFEDILEN FORMÃœL")
        print("="*80)
        print(f"Î“(t) = {best_formula}")
        print(f"RÂ² Score: {self.model.score(X, y):.4f}")
        print("="*80)
        
        # TÃ¼m adaylarÄ± gÃ¶ster
        print("\nTÃœM ADAY FORMÃœLLER (Complexity vs Accuracy):")
        print(self.model.equations_)
        
        return best_formula
    
    def predict(self, M, a, tau, epsilon):
        """KeÅŸfedilen formÃ¼lÃ¼ kullanarak tahmin yap."""
        X = np.column_stack([M, a, tau, epsilon])
        return self.model.predict(X)
```

**KullanÄ±m:**

```python
# main_symbolic_discovery.py

from models.symbolic_discovery import SymbolicGRM

# Baseline residuals
baseline = BaselineARIMA()
baseline.fit(train_df['y'])
residuals = baseline.get_residuals()

# Symbolic discovery
symbolic_grm = SymbolicGRM()
formula = symbolic_grm.discover_formula(residuals, window_size=20)

# Ã–rnek Ã§Ä±ktÄ±:
# Î“(t) = 0.523*M*tanh(epsilon) + 0.187*a*epsilon*exp(-0.05*tau)
```

**Avantajlar:**
1. **Data-Driven:** Veri kendi formÃ¼lÃ¼nÃ¼ yazÄ±yor
2. **Interpretable:** Sembolik formÃ¼l, yorumlanabilir
3. **Discovery:** Beklenmedik iliÅŸkiler keÅŸfedilebilir

**Beklenen Ã‡Ä±ktÄ±:**
```
================================================================================
KEÅFEDILEN FORMÃœL
================================================================================
Î“(t) = 0.523*M*tanh(epsilon) + 0.187*a*epsilon*exp(-0.05*tau) - 0.034*M^2
RÂ² Score: 0.8234
================================================================================

TÃœMADAY FORMÃœLLER (Complexity vs Accuracy):
   complexity                               equation     loss    score
0           5                    0.523*M*tanh(epsilon)  0.0234  0.7845
1           8      0.523*M*tanh(epsilon) + 0.187*a*epsilon  0.0198  0.8012
2          12  0.523*M*tanh(epsilon) + 0.187*a*epsilon*exp(-0.05*tau)  0.0165  0.8234
3          15  ... - 0.034*M^2  0.0163  0.8245
```

**Action Plan:**
1. PySR kur: `pip install pysr`
2. `models/symbolic_discovery.py` oluÅŸtur
3. `main_symbolic_discovery.py` oluÅŸtur
4. Sembolik regresyon Ã§alÄ±ÅŸtÄ±r (30-60 dakika)
5. KeÅŸfedilen formÃ¼lÃ¼ manual formÃ¼l ile karÅŸÄ±laÅŸtÄ±r

**SÃ¼re:** 3-5 gÃ¼n

---

### ğŸ”® **GELÄ°ÅTÄ°RME 4: N-Body Problem - Ã‡oklu Kara Delik**

**Ne:** Birden fazla ÅŸok kaynaÄŸÄ±nÄ± (Ã§oklu kara delik) modelle

**Teorik Temel:**
- N-body gravitational simulation
- Regime switching models
- Clustering algorithms

**Konsept:**

```
Tek Kara Delik (Mevcut):
    [â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€]
              M(t)

Ã‡oklu Kara Delik (Ã–nerilen):
    [â”€â”€â—â‚â”€â”€â”€â”€â—â‚‚â”€â”€â”€â”€â”€â”€â—â‚ƒâ”€â”€â”€â”€â”€â”€â”€]
       Mâ‚    Mâ‚‚       Mâ‚ƒ
    
    Î“_total(t) = Î£áµ¢ Î“áµ¢(t, Máµ¢, aáµ¢, Ï„áµ¢)
```

**Ä°mplementasyon:**

```python
# Yeni dosya: models/multi_body_grm.py

from sklearn.cluster import DBSCAN
import numpy as np

class MultiBodyGRM:
    """
    N-body GRM: Birden fazla gravitational anomaly.
    
    YaklaÅŸÄ±m:
    1. ArtÄ±klarÄ± farklÄ± rejimlere kÃ¼meleyerek ayÄ±r (DBSCAN, HMM)
    2. Her rejim = bir "kara delik"
    3. Her kara delik iÃ§in ayrÄ± parametreler (Mi, ai, Ï„i)
    4. Toplam etki = sÃ¼perpozisyon: Î“ = Î£áµ¢ Î“áµ¢
    """
    
    def __init__(self, n_bodies=3, window_size=20):
        self.n_bodies = n_bodies
        self.window_size = window_size
        self.body_params = []  # Her body iÃ§in (alpha, beta, gamma)
    
    def cluster_residuals(self, residuals):
        """
        ArtÄ±klarÄ± farklÄ± rejimlere ayÄ±r (clustering).
        
        Returns
        -------
        regime_labels : np.ndarray
            Her zaman noktasÄ± iÃ§in rejim etiketi (0, 1, 2, ...)
        """
        # Feature engineering: rolling statistics
        features = []
        for t in range(self.window_size, len(residuals)):
            window = residuals[t - self.window_size:t]
            features.append([
                np.mean(window),
                np.std(window),
                np.max(window),
                np.min(window),
                self.autocorr(window)
            ])
        
        features = np.array(features)
        
        # DBSCAN clustering
        clusterer = DBSCAN(eps=0.5, min_samples=10)
        regime_labels = clusterer.fit_predict(features)
        
        print(f"Tespit edilen rejim sayÄ±sÄ±: {len(np.unique(regime_labels))}")
        
        return regime_labels
    
    def fit_body(self, regime_residuals, body_id):
        """Bir body (rejim) iÃ§in parametreleri optimize et."""
        # Her body iÃ§in ayrÄ± SchwarzschildGRM fit et
        grm = SchwarzschildGRM(window_size=self.window_size)
        grm.fit(regime_residuals)
        
        params = {
            'body_id': body_id,
            'alpha': grm.alpha,
            'beta': grm.beta,
            'n_samples': len(regime_residuals)
        }
        
        self.body_params.append(params)
        return params
    
    def fit(self, residuals):
        """TÃ¼m body'leri fit et."""
        # Rejimleri tespit et
        regime_labels = self.cluster_residuals(residuals)
        
        # Her rejim iÃ§in ayrÄ± fit
        for regime_id in np.unique(regime_labels):
            if regime_id == -1:  # Noise points
                continue
            
            regime_mask = regime_labels == regime_id
            regime_residuals = residuals[self.window_size:][regime_mask]
            
            print(f"\nBody {regime_id} eÄŸitiliyor ({len(regime_residuals)} sample)...")
            params = self.fit_body(regime_residuals, regime_id)
            print(f"  alpha={params['alpha']:.4f}, beta={params['beta']:.4f}")
    
    def compute_curvature(self, residuals, current_regime):
        """
        Toplam bÃ¼kÃ¼lmeyi hesapla (sÃ¼perpozisyon).
        
        Î“_total = Î£áµ¢ wáµ¢ * Î“áµ¢
        
        wáµ¢: Her body'nin aÄŸÄ±rlÄ±ÄŸÄ± (gÃ¼ncel rejime uzaklÄ±ÄŸa gÃ¶re)
        """
        total_curvature = 0.0
        
        for params in self.body_params:
            body_id = params['body_id']
            alpha = params['alpha']
            beta = params['beta']
            
            # Weight: eÄŸer gÃ¼ncel rejim bu body'ye yakÄ±nsa, aÄŸÄ±rlÄ±k yÃ¼ksek
            if body_id == current_regime:
                weight = 1.0
            else:
                weight = 0.1  # DiÄŸer body'lerin zayÄ±f etkisi
            
            # Bu body'nin katkÄ±sÄ±
            mass = np.var(residuals[-self.window_size:])
            gamma_i = alpha * mass * np.sign(residuals[-1])
            
            total_curvature += weight * gamma_i
        
        return total_curvature
    
    def autocorr(self, x, lag=1):
        """Autocorrelation hesapla."""
        if len(x) <= lag:
            return 0.0
        return np.corrcoef(x[lag:], x[:-lag])[0, 1]
```

**Beklenen Ä°yileÅŸme:**
- Tek body'den %5-10 daha iyi
- FarklÄ± ÅŸok tÃ¼rlerini (pozitif/negatif, kÄ±sa/uzun) ayÄ±rt edebilir
- Rejim geÃ§iÅŸlerini yakalayabilir

**Action Plan:**
1. `models/multi_body_grm.py` oluÅŸtur
2. Clustering ve regime detection test et
3. Tek body vs Multi-body karÅŸÄ±laÅŸtÄ±r

**SÃ¼re:** 1-2 hafta

---

## ğŸ“‹ Ã–NERÄ°LEN UYGULAMA SIRASI

### **FAZE 4: ZenginleÅŸtirme (1-2 hafta)**
1. âœ… Decay factor ekle (Ã–ncelik 1) - 2 gÃ¼n
2. âœ… Ablasyon Ã§alÄ±ÅŸmasÄ± (Ã–ncelik 2) - 3 gÃ¼n
3. âœ… Time-series CV (Ã–ncelik 3) - 2 gÃ¼n
4. âœ… SonuÃ§larÄ± raporla ve analiz et - 1 gÃ¼n

**Beklenen SonuÃ§:** RMSE 0.102 â†’ 0.095 (%7 iyileÅŸme)

---

### **FAZE 5: PIML Entegrasyonu - Temel (2-3 hafta)**
1. âœ… GRN (Gravitational Residual Network) - 1 hafta
2. âœ… Manuel vs GRN karÅŸÄ±laÅŸtÄ±rmasÄ± - 2 gÃ¼n
3. âœ… Symbolic regression pilot - 3 gÃ¼n

**Beklenen SonuÃ§:** RMSE 0.095 â†’ 0.085 (%10 iyileÅŸme)

---

### **FAZE 6: PIML Ä°leri Seviye (1-2 ay)**
1. âœ… Unified end-to-end model - 2 hafta
2. âœ… Multi-body GRM - 1 hafta
3. âœ… KapsamlÄ± karÅŸÄ±laÅŸtÄ±rma ve raporlama - 1 hafta

**Beklenen SonuÃ§:** RMSE 0.085 â†’ 0.075 (%15 toplam iyileÅŸme)

---

## ğŸ“ AKADEMÄ°K YAYIM STRATEJÄ°SÄ°

### **YayÄ±n 1: Mevcut Sistem (HazÄ±r)**
**BaÅŸlÄ±k:** "Gravitational Residual Model (GRM): A Physics-Inspired Framework for Time Series Residual Modeling"

**Ä°Ã§erik:**
- Faze 1-3 sonuÃ§larÄ±
- Schwarzschild & Kerr rejimleri
- GARCH karÅŸÄ±laÅŸtÄ±rmasÄ±
- Ablasyon Ã§alÄ±ÅŸmasÄ±

**Hedef:** Time Series Analysis konferansÄ± veya dergisi

---

### **YayÄ±n 2: PIML Entegrasyonu (6 ay sonra)**
**BaÅŸlÄ±k:** "Physics-Informed Neural Networks for Gravitational Residual Modeling: Learning Curvature Functions from Data"

**Ä°Ã§erik:**
- GRN mimarisi
- Physics-informed loss
- Symbolic regression sonuÃ§larÄ±
- Manuel vs Ã¶ÄŸrenilmiÅŸ fonksiyon

**Hedef:** ICML, NeurIPS, ICLR (PIML workshop)

---

### **YayÄ±n 3: Unified System (1 yÄ±l sonra)**
**BaÅŸlÄ±k:** "End-to-End Gravitational Time Series Modeling: A Unified Framework Combining Baseline Forecasting and Residual Dynamics"

**Ä°Ã§erik:**
- Unified architecture
- Multi-body extensions
- KapsamlÄ± benchmark (GARCH, LSTM, Transformer)
- GerÃ§ek dÃ¼nya uygulamalarÄ±

**Hedef:** Nature Machine Intelligence, JMLR

---

## ğŸ“Š Ã–ZET: HIZLI KAZANIMLAR

### **BU HAFTA (1-2 gÃ¼n):**
1. Decay factor ekle â†’ %2-3 iyileÅŸme
2. Event horizon istatistiksel tanÄ±m â†’ Objektiflik

### **BU AY (2-3 hafta):**
1. Ablasyon Ã§alÄ±ÅŸmasÄ± â†’ Hangi bileÅŸen kritik?
2. Time-series CV â†’ SaÄŸlamlÄ±k testi
3. Ä°lk GRN denemesi â†’ %5-8 iyileÅŸme

### **3 AY (PIML pilot):**
1. GRN Ã¼retim sÃ¼rÃ¼mÃ¼ â†’ %10-12 iyileÅŸme
2. Symbolic discovery â†’ Yeni formÃ¼l keÅŸfi
3. Ä°lk makale hazÄ±rlÄ±ÄŸÄ±

### **6-12 AY (Tam PIML sistemi):**
1. Unified model â†’ %15-20 toplam iyileÅŸme
2. Multi-body extensions â†’ Rejim switching
3. KapsamlÄ± akademik yayÄ±n

---

## ğŸ¯ SONUÃ‡

**Mevcut Projeniz:** Metodolojik olarak saÄŸlam, iyi yapÄ±landÄ±rÄ±lmÄ±ÅŸ âœ…  
**Ana Limitasyon:** Manuel bÃ¼kÃ¼lme fonksiyonu, tek body varsayÄ±mÄ± âš ï¸  
**En Ã–nemli Ä°yileÅŸtirme:** GRN (Ã¶ÄŸrenilebilir fonksiyon) ğŸš€  
**Akademik Potansiyel:** Ã‡ok yÃ¼ksek (3 makale + PIML alanÄ±na katkÄ±) ğŸ“

**Ä°lk AdÄ±m Ã–nerisi:** Decay factor ekleyip ablasyon Ã§alÄ±ÅŸmasÄ± yapÄ±n. Bu, hÄ±zlÄ± kazanÄ±m saÄŸlar ve PIML'e geÃ§iÅŸ iÃ§in zemin hazÄ±rlar.

Hangi Ã¶nceliklendirmeyle baÅŸlamak istersiniz? ğŸ”§

